---
layout: post
title: ComSL - Composite Speech Language Model
date: 2024-04-17 11:12:00-0400
description: an example of a blog post with some math
tags: Language Speech Translation 
categories: ASR
related_posts: false
featured: true
---
*NOTE: This Blog is part of ASR course*

Are you tired of your speech-to-text translations sounding like they've been through a game of telephone with a mischievous gremlin? Fear not, because ComSL is here to save the day! Picture this: a speech-language model so revolutionary that it's like blending the perfect smoothie of pre-trained speech and language models, sprinkled with a touch of cross-modality learning magic. Forget the days of awkward mistranslations and GPU overload nightmares - ComSL is about to rock your linguistic world like never before! With a wink to data efficiency and a nod to multi-task learning, ComSL is the superhero we never knew we needed in the realm of end-to-end speech-to-text translation. So grab your popcorn and get ready for a wild ride through the wacky world of speech and language, because ComSL is about to take you on an adventure you won't soon forget!

### End to End Speech Models
E2E modeling enables training a single model through E2E optimization with task-oriented metrics. This approach has been widely applied to spoken language tasks in recent years like  speech translation, speech summarization, speech understanding etc. A conventional E2E model pipeline design generally consists of 2 modules :
1. Speech Model : decodes input speech into text
2. Language Model : infers recognized text to target language (translation task), topic sentence (summarization task) or intent (understanding task)

Point to be noted is that these 2 modules are trained using their own respective criteria, which may not be optimal for each other. Therefore, the cascases process may propagate errors that might have occured in a current module to the one following it. Also, since other information like prosodic features are contained in the speech, it becomes difficult to quantize them using language symbols, however they can be beneficial for spoken language tasks.

Therefore, unified speech language pretraining based on Transformer architecture has came into play and has largely boosted E2E modeling for spoken language tasks. In such models, pretraining is conducted jointly on unlabeled speech, unlabeled text, paired speech-to-text and paired text-to-text in multiple languages using both supervised and self-supervised learning. The unified representation from both modalities are simultaneously learned via shared model parameters or auxiliary modality matching losses in pretraining stage. But, as pretrained speech-only and language-only models are becoming increasing powerful, to achieve a comparable performance with such a cascaded module system, unified speech-language pretraining must leverage same or larger scale of data used in only models, which makes training very challenging.

### Aim of this Paper
ComSL: a Composite Speech-Language Model for spoken language tasks, which -
- fully leverages existing pretrained models, therefore, no need for pretraining with large data from scratch, you can just directly fine-tune it for downstream tasks. Hence, it is data efficient
- Conventional approaches use contrastive learning among modalities, which require external or internal aligners to force-align speech & text at token/word level. But ComSL's cross-modality learning with speech-text mapping/matching on either representations is only based on concatenation of paired speech & text. This simplifies implementation and allows it to be incorporated in fine-tuning stage
- Includes comprehensive ablation study on bridging gap of speech & languages and comparisons with previous works
- The model outperforms SOTA Google USM, OpenAI whisper ad cascaded non-E2E models by 0.8, 1.8, 1.0 average BLEU score improvements on CoVoST2 evaluation set respectively

## Method
### Problem Formulation
As we have seen now, the goal of E2E speech translation is to directly translate speech from a source language ($$ L_1 $$) into text in a target language ($$ L_2 $$), without generating an intermediate ASR transcription. Formally, we have to find the most likely sequence $$ y = {y_1, y_2, \ldots , y_N} $$ (eg. words or characters) of length $$ N $$ in $$ L_2 $$ given acoustic features $$ s = {s_1, s_2, \ldots , s_T} $$ (eg. Mel Filter Bank features) of length $$ T $$ in $$ L_1 $$. A Speech Translation (ST) corpus $$ D^{ST} = \{(s, x, y)\} $$